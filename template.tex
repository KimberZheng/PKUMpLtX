%
% PKUMpLtX --- A LaTeX document class for 'Modern Physics Laboratory' in PKU based on `revtex4-2`
%
% Please read `README.md' and the template file before using
% 需要确保 font 选项指定的字体已安装! 具体参见 `README.md' 的说明.
\documentclass[font=default]{mpltx}
\usepackage{booktabs}  
% 以下至 \begin{document} 都仅是本文件为了方便额外定义的命令, 写报告时不需要.
\hypersetup{colorlinks=true}% 超链接带颜色
\usepackage{xcolor}
\newcommand{\note}[1]{{\color{gray}#1}}
\NewDocumentCommand{\pkg}{s o m}{%
    \IfBooleanF{#1}{%
        \IfNoValueTF{#2}%
            {\href{https://www.ctan.org/pkg/#3}}%
            {\href{https://www.ctan.org/pkg/#2}}%
    }%
    {\textsf{#3}}%
}
\newcommand*\cs[1]{\texttt{\textbackslash #1}}
\newcommand*\env[1]{\textit{\texttt{#1}}}
\newcommand*\code[1]{\texttt{#1}}
\newcommand*\file[1]{\textbf{\texttt{#1}}}
\makeatletter
\newcommand\releasedate{%
    \href{https://github.com/CastleStar14654/PKUMpLtX/releases/tag/\mpltx@fileversion}%
        {\mpltx@filedate, \mpltx@fileversion}}
\makeatother
% 以上是本文件为了方便额外定义的命令, 写报告时不需要.

\begin{document}
\title{具身只能与通用智能体个人报告} % 切合报告内容, 简短明确, 可以不同于讲义
\author{郑熔} % 这里 \emailphone 一定要紧跟在 \author 后方
\emailphone{2300011359@stu.pku.edu.cn}{(86)19805861588}
% 如果改用 \email 则仅需要邮箱参数
\affiliation{北京大学物理学院\quad 学号: 2300011359}
% % 可以使用 \zhdate 自动生成中文日期, 如
\date{\zhdate{2025/12/18}}
% % 也可使用 babel 的 \localedate, 如
% \date{\localedate{2020}{12}{1}}
% % 两者均会输出 `2020 年 12 月 1 日'
% 下面的 \date 的参数是为了自动输出正确版本号, 正式报告请替换为上面的两种 \date 之一
% \date{\releasedate}

\maketitle
\section{项目概况、 硬件与环境介绍}
\subsection{第一、二次作业}
\subsubsection{物理端}
乐聚机器人，全身包括17个舵机，第二次作业还用到了风扇、视觉传感器（胸部的相机等）。
\subsubsection{虚拟端}
Aelos\_edu示教器，也可以使用VSCode或者MobaXterm SSH连接机器人的代码库。
\subsection{第三次作业}

\subsection{物理端}
笔记本电脑.

\subsection{虚拟端}
虚拟人开发环境包括Ollama、Fay、Fay UE5等。
还需要在在Llama Factory中进行模型微调。



\section{模块一： 机器人的基础运动}
\subsection{动作原理分析}
机器人的运动是通过舵机的转动来实现的。每个舵机可以控制一个关节的角度，通过协调多个舵机的动作，可以实现机器人的行走、转向等动作。
此外，还需要考虑重心的变化、力矩的平衡和步态的设计，以确保机器人在运动过程中保持稳定。


例如，“向前伸手”动作中，我们需要控制机器人的左侧手臂，从下垂状态抬起，向前伸出再收回。
“向前一步”动作则被分解为如下几个步骤：重心偏移到右腿，抬起左腿，向前摆左腿，左腿落地，重心前移，重心转到左腿，抬起右腿，向前摆右腿，右腿落地。
“向左移动一步”动作中，动作分解如下：重心偏移至右腿，抬起左腿，向左摆动左腿，并落地，在左腿移动过程中，重心左移；重心偏移至左腿，抬起右腿，向左摆动右腿，
并落地，重心右移，最终机器人恢复站立姿态。“弯腰”动作则被分解为如下几个步骤：撅起屁股并弯曲膝盖，弯上身，再回到撅屁股和弯膝盖状态，最后回到站立状态。

其中，“撅屁股”
是因为机器人的上身非常重，直接弯腰会导致力矩不平衡，容易摔倒。


\subsection{核心代码逻辑}
更改代码库中的json文件可以达到更改舵机角度的效果，但是实在是过于复杂。我们采取的是先打开示教器，操作舵机，由此可以自动更改代码库中的舵机角度，
再SSH连接代码库，运行下面的代码，即可。
\begin{verbatim}
  import sys
  sys.path.append("/home/sunrise/catkin_ws/src/aelos_smart_ros")
  from leju import *
  def main():
      nodes.node_initial()
      try:
          base_action.action('弯腰')
          base_action.action('左右腿2')
          base_action.action('抬手')
          base_action.action('向左一步')
      
      except Exception as e:
          nodes.serror(e)
          exit(2)
      finally:
          nodes.finishsend()

  if __name__ == "__main__":
      print ("Run custom project")
      main()
\end{verbatim}

\subsection{成果展示}

我们所有的成果均附在第一次作业的报告中，这里仅展示“向左移动一步”动作的截图作为示例，如图\autoref{fig:3}所示。

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{fig/向左移动一步.png}
  \caption{机器人做“向左移动一步”动作}
  \label{fig:3}
\end{figure}

\subsection{我的分工}
我在这个环节中主要参与了动作设计、代码库的文件寻找、视频拍摄等工作。
并且发现，使用VSCode连接比使用MobaXterm连接更为方便。
因为代码库文件过多，难以找到main.py文件，但如果使用VSCode连接，可以直接在左侧栏搜索文件名，或者在示教器更改舵机角度后，VSCode会直接有颜色显示哪里有所更改。
这样定位需要运行的代码文件位置更方便。

\section{模块二：机器人的实战操作}

\subsection{视觉感知算法}
HSV（Hue-Saturation-Value）算法是机器人视觉感知核心技术，通过将图像从 RGB 空间转换为色相、饱和度、明度三个独立维度，降低光照干扰，
可实时捕捉目标物体、场地边界等关键信息（这一句话是由AI生成）。

\subsection{动态策略设计}

识别到多物体后，随机向一个目标移动。
先左右移动，后前后移动，以对准目标。
前后移动过程中，距离较远时，采用快走3步，距离中等时，快走1步，距离较近时慢走1步。
每进行一步操作，实时更新位置一次。

\subsection{任务集成}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{fig/HSV.png}
  \caption{火焰识别效果图}
  \label{fig:4}
\end{figure}

\begin{enumerate}
    \item 打开视觉回传，采样火焰颜色，获得HSV的范围，如\autoref{fig:4}可以看到，识别准确。
    \item 调用胸部的相机，使用colour\_port.get\_central\_coordinat（）模块识别视野中的红色物体，得到物体中心的x坐标和y坐标。其中，x坐标表示的是火焰的左右位置，y坐标表示的是前后位置。坐标系以左上角为原点，x正方向向右，y正方向向下。x越大，则火焰越靠视野的右方；y越小，则火焰离机器人越远。
    \item 移动机器人，使火焰位置移动到机器人视野中心。若x坐标大于328，则向右移动，若小于320，则向左移动。
    \item 机器人向前走，逐渐靠近火焰。在距离火焰较远的地方，y坐标小于100，则快走三步；稍微靠近后，在y坐标小于300，则快走一步；再靠近一些，y坐标在300~330之间时，慢走一步。
    \item 机器人站在火焰附近后，会自动使用代码sensor\_port.set\_output(2, 1)指令开启风扇，迅速移走火焰物体，模仿火焰熄灭过程。代码time.sleep(5)使得风扇转动5秒后自动停止。
    \item 熄灭火焰后，机器人继续识别视野中是否有火焰，若有，则重复上述步骤，直至视野中无火焰为止。
\end{enumerate}


其中，火焰颜色的识别受环境光照影响较大，更换实验场景后，需要重新采样火焰颜色，获得新的HSV范围。
采用320-328作为x坐标的容差范围，是因为机器人在移动过程中，存在一定的抖动，无法完全稳定在某一位置。


我们实现的流程中，机器人在重新识别火焰时只能识别目前视野中的火焰，而无法识别整个环境中的火焰。
如果想要实现这一点，可以考虑让机器人在每次熄灭火焰后，旋转一圈，扫描周围环境，寻找新的火焰目标。
旋转的过程对于我们的机器人来说并不是很容易的动作，因为他没有踝关节。我们可以采用：先把重心放在左脚，抬起右脚，向右前方略迈步，中心落地后把重心放在右脚，抬起左脚，向后方略迈步，这样可以旋转微笑的角度。
以此类推，可以实现机器人原地旋转360度，扫描周围环境。

\subsection{我的分工}
在这部分中，我参与了机器人动作的调试、视频的拍摄等工作。并且，我发现了Aelos\_edu示教器的一个问题。循环模块其在python代码中的呈现有问题。
在示教器中其逻辑是“while (not A) and (not B)”在其代码库中的呈现是“while not A and not B”, 而 not 的优先级比 and 高，这会使得我们的机器人一直只能朝单一方向移动。
修改代码后，这一动作被修正。

\section{模块三：认知与多模态交互}

\subsection{系统架构图}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{fig/structure.png}
  \caption{系统架构图}
  \label{fig:5}
\end{figure}

系统架构图如图\autoref{fig:5}所示。首先，我们需要生成一个含有北京大学相关信息的数据集，包括饮食、学科、景点、历史四个方向，共九百多条数据。
其次，我们需要基于这些数据训练DeepSeek-R1-1.5B-Distill模型，使其能够回答与北京大学相关的问题。
然后我们需要将训练好的模型部署到本地的Ollama服务器上。为了集成语音识别和语音合成模块，
我们使用阿里云和火山引擎提供的API接口，将其部署到Fay中，以实现语音交互功能。
除此之外，为了实现可视化的虚拟人形象，我们使用Unreal Engine引擎的UE5虚拟人插件，创建一个虚拟人形象，并将其与Fay进行集成。
最后，我们实现了和虚拟人的多模态交互功能，用户可以通过语音与虚拟人进行交流，虚拟人可以通过语音和文本的方式回答用户的关于北京大学的各种问题，以实现北京大学导游的功能。


\subsection{模型的微调}
本实验采用 Llama Factory 框架对开源模型 DeepSeek-R1-1.5B-Distill 进行微调。
为了使通用的预训练模型能够习得北京大学相关知识，并保持导游角色的特定语气，我们进行了参数设计与实验。


由于模型规模（1.5B）与数据规模限制，
本实验采用了LoRA轻量化微调技术。
相比于全量微调，LoRA 通过在原始模型旁路注入可训练的低秩矩阵，
既大幅降低了显存占用，又有效避免了大规模训练导致的遗忘。


微调过程中的具体参数配置如表 \autoref{fig:parameters} 所示：

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/parameter.png}
    \caption{模型微调核心参数配置表}
    \label{fig:parameters}
\end{figure}

在此列举几个比较重要的参数：


\begin{enumerate}
    \item 模型选择 DeepSeek-R1-1.5B-Distill，体积适中，适合小样本微调；
    \item learning\_rate 设置为 2e-5，适合小样本微调；我们一开始尝试了5e-5，发现loss下降非常地陡峭波动，改为2e-5后，loss曲线下降较快、较为平稳；
    \item 采用LoRA微调技术，rank 设置为 32，可以在保证模型性能的同时，降低显存占用. 一开始我们尝试rank=8，但发现训练效果不好. 更高地rank可以提升模型的表达能力；
    \item LoRA的dropout 设置为 0.05，可以丢弃部分神经元，防止过拟合；
    \item batch\_size 设置为 2，结合梯度累积（gradient\_accumulation\_steps=8）可以模拟大批次训练；
    \item 训练的epochs 设置为 3，由于参数比较少，可以防止过拟合。
    \item 采用余弦退火学习率调度（cosine\_annealing）策略，可以在训练后期降低学习率，提升模型性能。
    \item warmup\_steps 设置为 10，一开始学习率较低，训练初期逐步增加学习率，防止梯度爆炸。
\end{enumerate}


最后训练得到的loss曲线成功下降，如图\autoref{fig:loss}所示。
可以看到loss在前几个epoch下降较快，后续趋于平稳，表明模型已较好地拟合了数据集特征。


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/loss.png}
    \caption{Loss 曲线}
    \label{fig:loss}
\end{figure}


微调后，导出模型并部署到本地 Ollama。和他对话时，可以看到模型已经掌握了北京大学的相关知识，并且能够以导游的语气进行回答，如图\autoref{fig:chat}所示。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/chat.png}
    \caption{微调后模型的对话示例}
    \label{fig:chat}
\end{figure}

最后，我们将微调后的模型集成到Fay中，实现了语音交互功能。
用户可以通过语音和文字与虚拟人进行交流，虚拟人可以通过语音和文本的方式回答用户的关于北京大学的各种问题，
以实现北京大学导游的功能。

具体结果见小组的报告视频。




\subsection{我的分工}
在这部分中，我负责了部分数据集的人工收集整理，并且辅助了一些模型部署的过程，
例如如何具体部署Ollama、Fay和UE5的资料查找，语音识别模块API的接入，
并负责最后成果的展示和报告。


\section{综合思考与总结}
控制实体机器人是现实世界中具体可控的，难点主要在于控制其物理平衡，设计其动作分解。
而控制虚拟人则具有数据交互特性，难点主要在于配置环境，设计其训练数据集。

开发过程中遇到最棘手的2个Bug:
\begin{enumerate}
    \item 在作业一中，直接让机器人向左一步过程较难控制机器人平衡。因为其重心不稳定，抬起的左腿没有经过中间步骤直接落地，很容易在左腿跨出的过程中就失去平衡跌倒。重心先不随着腿的位置而改变，而是始终保持重心在右腿，到左腿落地后再进行重心的转移。这样可以保持平衡。
    \item 在作业二中，机器人始终朝着一个方向移动，无法调整方向。经过排查，发现是因为示教器中循环模块的逻辑在代码库中呈现有误，在示教器中其逻辑是“while (not A) and (not B)”在其代码库中的呈现是“while not A and not B”, 而 not 的优先级比 and 高，这会使得我们的机器人一直只能朝单一方向移动。
修改代码后，这一动作被修正。
\end{enumerate}

收获与感想:
本次课程让我对于具身智能有了一个基本的认识，简单了解了一下机器人控制的基本原理、具身智能传感与控制、机器人环境交互建模、仿真系统等方面，另外还入门了一下机器学习基础。
通过操作实体机器人，完成一些简单的任务，我对于机器人、具身智能有了一个初步的认识。
操作虚拟人的过程则让我体会到虚拟人环境配置的复杂性。


这门课程是让零基础同学简单了解具身智能的一门比较好的入门课程。
对课程的建议：老师在讲解知识的时候可以不要讲得太过泛化，可以更细致更实用一些，例如具体公式的推导、具体模型内部的架构等等；课程设置可以再引人入胜一些。


\end{document}
